{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMaf+WS4NrC6QVN2BX1K/yz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hassan-DS507/data-science-notebooks/blob/main/Task_1_for_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Tom Clinic Data Cleaning Project\n",
        "\n",
        "##  Objective\n",
        "Clean and prepare the provided clinic dataset to make it ready for analysis by:\n",
        "- Handling missing values\n",
        "- Fixing inconsistent text entries\n",
        "- Removing duplicates\n",
        "- Ensuring correct data types"
      ],
      "metadata": {
        "id": "ojfDgBkaODZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 1: Import Necessary Libraries"
      ],
      "metadata": {
        "id": "kXKCF9RMONie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "hvFw3P2nONQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pbc2BYw1Yba5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkrO1mDReeoh"
      },
      "source": [
        "# Step 2: Load the Datasetet\n",
        "We load the dataset and take an initial look at its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdhXr1zjWRAH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_kJePAtNpIz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/tasks_to_dataset/01JT7BFHK057AQS04QAAWHCWNX (1).csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "id": "mUzumve9Rk63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Explore the Data"
      ],
      "metadata": {
        "id": "8jquQU-YUr3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "0loUd1nsR9ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "9AeLyzLFSEdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "xWHkGxQgSLli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Observation Summary:\n",
        "The dataset contains a high number of unique invoices and dates, indicating many transactions over time. Most categorical columns like Product, Brand, Branch, and Payment_Method have limited unique values, reflecting a controlled and structured retail environment.\n"
      ],
      "metadata": {
        "id": "sXTIYMOnUBQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "aCNR55mZU7j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data Summary and Observations\n",
        "\n",
        "###  General Info:\n",
        "- The dataset contains **2600 rows** and **10 columns**.\n",
        "- This is sales data, including invoice info, products, prices, customers, and payment methods.\n",
        "\n",
        "-  `Invoice_ID`: OK – All values are present and unique.\n",
        "-  `Date`: Type is text, needs to be converted to datetime.\n",
        "- `Customer_Name`: 333 missing – can fill with \"Unknown\".\n",
        "-  `Product`: OK – No missing values.\n",
        "-  `Brand`: OK – No missing values.\n",
        "-  `Quantity`: OK – Numeric and complete.\n",
        "-  `Unit_Price`: OK – Numeric and complete.\n",
        "-  `Branch`: OK – No missing values.\n",
        "-  `Payment_Method`: 608 missing – fill with \"Not Recorded\".\n",
        "-  `Total_Price`: OK – Numeric and complete.\n",
        "\n",
        "###  Next Steps:\n",
        "- Clean missing values.\n",
        "- Convert `Date` to datetime.\n",
        "- Validate that `Total_Price = Quantity × Unit_Price`."
      ],
      "metadata": {
        "id": "vT_toasMWggv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing = df.isna().sum().sort_values(ascending = False)\n",
        "print(f'Total Number of missing values in the Dataset {missing.sum()}\\n')\n",
        "missing = missing[missing>0]\n",
        "missing"
      ],
      "metadata": {
        "id": "xm_xg-oYXAHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Missing Values\n",
        "\n",
        "-  **Total Missing Values**: 941\n",
        "- `Payment_Method`: 608 missing\n",
        "  - Observation: Many transactions have no recorded payment method.\n",
        "\n",
        "  \n",
        "-  `Customer_Name`: 333 missing\n",
        "  - Observation: Some invoices are missing customer names.\n",
        "\n",
        "  - Action: you can check pattern\n",
        "\n",
        " All other columns have **zero missing values** — data is mostly clean.\n"
      ],
      "metadata": {
        "id": "kMW80-eXWlC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze the pattern of missing data in `Customer_Name` and `Payment_Method`"
      ],
      "metadata": {
        "id": "MxTxmSx0aigA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Check_Pattern(missing_df, null_col):\n",
        "    \"\"\"\n",
        "    Analyze the pattern of missing data in a specific column.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    missing_df : pandas.DataFrame\n",
        "        Rows where the specified column is missing.\n",
        "    null_col : str\n",
        "        The column with missing values.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Columns that tend to take only 1 or 2 unique values\n",
        "        when `null_col` is missing — this may indicate a non-random pattern.\n",
        "    \"\"\"\n",
        "    # Remove the target column (we don't need to analyze it here)\n",
        "    missing_df = missing_df.drop(null_col, axis=1)\n",
        "\n",
        "    # Create an empty DataFrame to store unique values for relevant columns\n",
        "    unique_df = pd.DataFrame()\n",
        "\n",
        "    for col in missing_df.columns:\n",
        "        nunique = missing_df[col].nunique()\n",
        "\n",
        "        if nunique in [1, 2]:  # If only 1 or 2 unique values exist\n",
        "            unique_vals = missing_df[col].unique()\n",
        "            unique_df[col + '_unique_vals'] = pd.Series(unique_vals)\n",
        "\n",
        "    # Interpretation\n",
        "    if unique_df.empty:\n",
        "        print(f'\\t- Missing values in `{null_col}` appear to be randomly distributed.')\n",
        "        print(f'\\t- Likely missing mechanism: MCAR (Missing Completely At Random)')\n",
        "    else:\n",
        "        print(f'\\t- Missing values in `{null_col}` are associated with specific values in other columns:')\n",
        "        print(f'\\t  Columns with 1 or 2 unique values when `{null_col}` is missing: {list(unique_df.columns)}')\n",
        "        print(f'\\t- Likely missing mechanism: MAR or MNAR (Not Missing Completely At Random)')\n",
        "\n",
        "    return unique_df\n",
        "\n",
        "\n",
        "def Missing_Pattern(df, col):\n",
        "    \"\"\"\n",
        "    Show stats and pattern analysis for a column with missing values.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "    col : str\n",
        "        Column name to analyze\n",
        "    \"\"\"\n",
        "    print(f\"\\nFeature: {col}\")\n",
        "    print('-'*40)\n",
        "    print(f\"\\t- Number of missing values: {df[col].isna().sum()}\")\n",
        "    print(f\"\\t- Percentage of missing values: {(df[col].isna().mean())*100:.2f}%\")\n",
        "    print(f\"\\t- Data type: {df[col].dtype}\")\n",
        "    print(f\"\\t- Number of unique values: {df[col].nunique(dropna=True)}\")\n",
        "    print(f\"\\t- Most common value: {df[col].mode(dropna=True).iloc[0] if df[col].notna().any() else 'N/A'}\")\n",
        "\n",
        "    print(f\"\\nAnalyzing missing value pattern...\")\n",
        "    print('-'*40)\n",
        "    missing = df[df[col].isna()]\n",
        "    unique_df = Check_Pattern(missing, col)\n",
        "    return missing, unique_df\n"
      ],
      "metadata": {
        "id": "iSImBeYcXxz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing1, pattern1 = Missing_Pattern(df, 'Customer_Name')\n",
        "missing2, pattern2 = Missing_Pattern(df, 'Payment_Method')"
      ],
      "metadata": {
        "id": "G0Th5VxlWjYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Values Analysis Summary\n",
        "\n",
        "- **Customer_Name**\n",
        "  - ~12.8% missing values\n",
        "  - Missing Completely At Random (MCAR)\n",
        "  -  Action: Fill missing values with `'Unknown'` to keep the data and track anonymous customers.\n",
        "\n",
        "- **Payment_Method**\n",
        "  - ~23.4% missing values\n",
        "  - Missing Completely At Random (MCAR)\n",
        "  -  Action: Fill missing values with the most common value (`mode`), e.g., `'Mobile Wallet'`.\n",
        "\n",
        "###  Final Decision:\n",
        "- No rows will be dropped.\n",
        "- Missing values will be imputed to retain useful sales data for analysis."
      ],
      "metadata": {
        "id": "_Ybsi37HcCnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Customer_Name'].fillna('Unknown', inplace=True)"
      ],
      "metadata": {
        "id": "iWDDp8sfcB-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Customer_Name'].isna().sum()"
      ],
      "metadata": {
        "id": "3CMVDNxLbk1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Payment_Method'].fillna(df['Payment_Method'].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "8TN-cEA5ccjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Payment_Method'].isna().sum()"
      ],
      "metadata": {
        "id": "DibD3oy2ccjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle duplicates"
      ],
      "metadata": {
        "id": "saCsJC01czAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "WkfQSkvRcnqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fix column data types if needed"
      ],
      "metadata": {
        "id": "S9F6p3hQdc9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "ZyxKBdFldfOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observation : Convert `Date` column\n"
      ],
      "metadata": {
        "id": "eznY26Yodwdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'].unique()"
      ],
      "metadata": {
        "id": "S3Db136veG5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def clean_date(date_str):\n",
        "    if pd.isnull(date_str):\n",
        "        return None\n",
        "\n",
        "    # Replace / with - for consistency\n",
        "    date_str = str(date_str).replace('/', '-')\n",
        "\n",
        "    # Possible date formats (more can be added if needed)\n",
        "    formats = ['%Y-%m-%d', '%d-%m-%Y', '%m-%d-%Y', '%d-%b-%Y', '%Y/%m/%d', '%d/%m/%Y']\n",
        "\n",
        "    for fmt in formats:\n",
        "        try:\n",
        "            date_obj = datetime.strptime(date_str, fmt)\n",
        "            return date_obj.strftime('%Y-%m-%d')  # Standardize format\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return None  # If all parsing fails\n",
        "\n",
        "# Example: apply on a column called 'Date'\n",
        "df['Date'] = df['Date'].apply(clean_date)\n"
      ],
      "metadata": {
        "id": "alQD3UM2orxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When I looked at the column for dates, I noticed that all the dates were not in the same format. This inconsistency is a significant issue.\n",
        "\n",
        "We will aim to resolve this in the future."
      ],
      "metadata": {
        "id": "2h4nbMlPgnE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n"
      ],
      "metadata": {
        "id": "5Tecykuvhuh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'].isna().sum()"
      ],
      "metadata": {
        "id": "ddmdlT9Ypox8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing = df.isna().sum().sort_values(ascending = False)\n",
        "print(f'Total Number of missing values in the Dataset {missing.sum()}\\n')\n",
        "missing = missing[missing>0]\n",
        "missing"
      ],
      "metadata": {
        "id": "RyOahyZRtME0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"cleaned_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "1URQp2R4pq0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "2UMdUoVWujZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}